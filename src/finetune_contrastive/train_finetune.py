import os
import sys
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, Dataset
from transformers import CLIPProcessor, CLIPModel
from torch.optim import AdamW
from PIL import Image

# --- 1. è·¯å¾„è®¾ç½®ä¸å¯¼å…¥ ---
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

try:
    from baseline.data import Flickr8kDataset
except ImportError:
    print("é”™è¯¯ï¼šæ‰¾ä¸åˆ° baseline.data æ¨¡å—ã€‚è¯·ç¡®ä¿ä»£ç ä½äº src ç›®å½•ä¸‹ã€‚")
    sys.exit(1)


# --- 2. Dataset å®šä¹‰ ---
class FinetuneDataset(Dataset):
    def __init__(self, dataframe, processor):
        self.df = dataframe.reset_index(drop=True)
        self.processor = processor

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        image_path = row['image_path']
        text = row['token']

        try:
            image = Image.open(image_path).convert("RGB")
        except Exception:
            # å®¹é”™ï¼šå¦‚æœå›¾ç‰‡è¯»ä¸å‡ºæ¥ï¼Œéšæœºè¯»ä¸‹ä¸€å¼ 
            return self.__getitem__((idx + 1) % len(self))

        inputs = self.processor(
            text=[text],
            images=image,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=77
        )

        return {
            "input_ids": inputs["input_ids"].squeeze(0),
            "attention_mask": inputs["attention_mask"].squeeze(0),
            "pixel_values": inputs["pixel_values"].squeeze(0)
        }


# --- 3. Loss å‡½æ•° ---
def contrastive_loss(logits_per_image, logits_per_text):
    batch_size = logits_per_image.shape[0]
    labels = torch.arange(batch_size).to(logits_per_image.device)
    loss_img = nn.functional.cross_entropy(logits_per_image, labels)
    loss_txt = nn.functional.cross_entropy(logits_per_text, labels)
    return (loss_img + loss_txt) / 2


# --- 4. è‡ªåŠ¨ç”»å›¾å‡½æ•° ---
def plot_loss_curve(steps, losses, save_path="finetune_loss.png"):
    print(f"æ­£åœ¨ç”Ÿæˆ Loss æ›²çº¿: {save_path} ...")
    plt.figure(figsize=(10, 6), dpi=100)

    plt.scatter(steps, losses, alpha=0.2, color='gray', s=5, label='Raw Step Loss')

    window_size = 20
    if len(losses) > window_size:
        smooth_losses = np.convolve(losses, np.ones(window_size) / window_size, mode='valid')
        smooth_steps = steps[window_size - 1:]
        plt.plot(smooth_steps, smooth_losses, color='#1f77b4', linewidth=2, label='Smoothed Trend')
    else:
        plt.plot(steps, losses, color='#1f77b4', linewidth=2, label='Loss')

    plt.title('CLIP Finetuning Training Loss', fontsize=14, fontweight='bold')
    plt.xlabel('Training Steps', fontsize=12)
    plt.ylabel('InfoNCE Loss', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend()
    plt.tight_layout()
    plt.savefig(save_path)
    print(f"Loss æ›²çº¿å·²ä¿å­˜ï¼")


# --- 5. ä¸»è®­ç»ƒæµç¨‹ ---
def main():
    # é…ç½®
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model_name = "openai/clip-vit-base-patch32"
    batch_size = 32
    epochs = 3
    lr = 1e-5
    save_path = "./finetuned_model"

    # === ğŸŸ¢ å…³é”®è®¾ç½®ï¼šæµ‹è¯•é›†å¤§å° ===
    TEST_SIZE = 1000  # ä¿ç•™æœ€å 1000 æ¡ä¸å‚ä¸è®­ç»ƒ
    # ============================

    print(f"æ­£åœ¨ä½¿ç”¨è®¾å¤‡: {device}")

    # A. åŠ è½½å…¨é‡æ•°æ®
    print("æ­£åœ¨åŠ è½½æ•°æ®...")
    img_dir, token_path = Flickr8kDataset.get_path()
    dataset_handler = Flickr8kDataset(img_dir, token_path)
    df = dataset_handler.load_dataset()
    df = dataset_handler.validate_dataset(df)

    print(f"åŸå§‹æ•°æ®æ€»é‡: {len(df)}")

    # === ğŸŸ¢ å…³é”®æ­¥éª¤ï¼šå‰”é™¤æµ‹è¯•é›† ===
    # df.iloc[:-1000] æ„æ€æ˜¯å–ä»å¤´å¼€å§‹ç›´åˆ°å€’æ•°ç¬¬1000ä¸ª
    train_df = df.iloc[:-TEST_SIZE]

    print(f"-" * 30)
    print(f"è®­ç»ƒé›†æ•°é‡: {len(train_df)} (ç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°)")
    print(f"æµ‹è¯•é›†æ•°é‡: {TEST_SIZE} (ä¿ç•™ç”¨äºåç»­ retrieval è¯„ä¼°)")
    print(f"æ³¨æ„ï¼šæ¨¡å‹å°†å®Œå…¨ä¸ä¼šçœ‹åˆ°è¿™æœ€å {TEST_SIZE} æ¡æ•°æ®ï¼")
    print(f"-" * 30)
    # ============================

    # B. åˆå§‹åŒ–æ¨¡å‹
    processor = CLIPProcessor.from_pretrained(model_name)
    try:
        model = CLIPModel.from_pretrained(model_name, use_safetensors=True).to(device)
    except:
        print("Safetensors åŠ è½½å¤±è´¥ï¼Œå°è¯•é»˜è®¤åŠ è½½...")
        model = CLIPModel.from_pretrained(model_name).to(device)

    # C. å†»ç»“å‚æ•° (åªè®­ç»ƒ Projection å±‚)
    for name, param in model.named_parameters():
        if "projection" in name or "layer_norm" in name:
            param.requires_grad = True
        else:
            param.requires_grad = False

    # D. å‡†å¤‡ DataLoader (åªä½¿ç”¨ train_df)
    train_ds = FinetuneDataset(train_df, processor)
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)

    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)

    # è®°å½•ç»˜å›¾æ•°æ®
    history_steps = []
    history_losses = []
    global_step = 0

    # E. è®­ç»ƒå¾ªç¯
    model.train()
    print("å¼€å§‹è®­ç»ƒ...")

    for epoch in range(epochs):
        total_loss = 0
        for step, batch in enumerate(train_loader):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            pixel_values = batch["pixel_values"].to(device)

            optimizer.zero_grad()

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                pixel_values=pixel_values,
                return_loss=True
            )

            loss = contrastive_loss(outputs.logits_per_image, outputs.logits_per_text)
            loss.backward()
            optimizer.step()

            current_loss = loss.item()
            total_loss += current_loss

            history_steps.append(global_step)
            history_losses.append(current_loss)
            global_step += 1

            if step % 50 == 0:
                print(f"Epoch {epoch + 1}, Step {step}, Loss: {current_loss:.4f}")

        avg_loss = total_loss / len(train_loader)
        print(f"=== Epoch {epoch + 1} å®Œæˆ, å¹³å‡ Loss: {avg_loss:.4f} ===")

    # F. ä¿å­˜æ¨¡å‹
    print(f"æ­£åœ¨ä¿å­˜æ¨¡å‹åˆ° {save_path}...")
    model.save_pretrained(save_path)
    processor.save_pretrained(save_path)

    # G. è‡ªåŠ¨ç”»å›¾
    plot_loss_curve(history_steps, history_losses, save_path="finetune_loss.png")

if __name__ == "__main__":
    main()