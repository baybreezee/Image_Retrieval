CLIP Fine-tuning for Image Retrieval on Flickr8kThis project implements a Contrastive Fine-tuning pipeline for the CLIP model (ViT-B/32) to improve natural language image retrieval performance on the Flickr8k dataset.Key features:Strict Data Splitting: Train (95%) / Validation (2.5%) / Test (2.5%) to prevent data leakage.Deep Unfreezing: Fine-tunes the Projection Layer and the last Vision Transformer layer.
Corrected Evaluation: Uses path-based matching to handle one-to-many ground truth mappings.üõ†Ô∏è RequirementsEnsure you have the following dependencies installed:Bashpip install torch transformers pillow numpy matplotlib tqdm scikit-learn
Usage WorkflowFollow these three steps to reproduce the results.Step 1: Train the ModelRun the training script. This script uses a three-way data split, applies cosine learning rate scheduling, and automatically saves the best model based on Validation Loss.Config: Batch Size = 32, LR = 1e-6, Epochs = 15.Strategy: Unfreeze Last ViT Layer + Projection Layer.Bashpython src/train_ultimate_no_aug.py
Note: The best model will be saved to ./finetuned_model_best.Step 2: Generate Feature VectorsLoad the fine-tuned model and generate image/text embeddings for the held-out Test Set (last 1,000 samples).Open src/generate_vectors.py.Ensure model_path points to ./finetuned_model_best.Run the script:Bashpython src/generate_vectors.py
Output: Vectors will be saved to data/embedding_finetuned_best.Step 3: Evaluation (Retrieval)Calculate Recall@K metrics (R@1, R@5, R@10) using the generated vectors.Open src/retrieval_finetuned.py.Ensure EMBEDDING_DIR points to data/embedding_finetuned_best.Run the evaluation:Bashpython src/retrieval_finetuned.py
ResultsPerformance on the held-out Test Set (1,000 samples):MetricBaseline (Zero-shot)Ours (Fine-tuned)Recall@176.30%82.80%Recall@576.30%82.80%Recall@1086.20%92.50%
