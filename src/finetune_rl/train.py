# # ==================== train_rl.py (RL-inspired Rank-Aware Adapter) =====================

import os
import random
import numpy as np
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

import clip

from .reward import compute_rank_and_reward
from .utils import load_embeddings, recall_at_k


# ---------------- dataset ----------------
class TextImageRLDataset(Dataset):
    def __init__(self, text_items):
        self.text_items = list(text_items)

    def __len__(self):
        return len(self.text_items)

    def __getitem__(self, idx):
        # idx åŒæ—¶ä½œä¸º â€œæ­£ç¡®å›¾ç‰‡â€ åœ¨ image_embs ä¸­çš„ç´¢å¼•
        return idx, self.text_items[idx]


# ---------------- small adapter (åªè®­ç»ƒå®ƒ) ----------------
class RankAdapter(nn.Module):
    def __init__(self, dim=512):
        super().__init__()
        self.linear = nn.Linear(dim, dim, bias=False)
        # åˆå§‹åŒ–ä¸ºæ¥è¿‘æ’ç­‰æ˜ å°„ï¼Œé¿å…ä¸€å¼€å§‹å°±æŠŠç©ºé—´æå
        nn.init.eye_(self.linear.weight)

    def forward(self, x):
        return self.linear(x)


# ---------------- seed ----------------
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


# ---------------- main train ----------------
def main():
    set_seed(42)
    
    # ===== è¶…å‚åŒºåŸŸ =====
    NUM_EPOCHS = 10          
    LR = 5e-5               
    ALPHA = 0.5             
    L2_LAMBDA = 1e-4        

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("Using device:", device)

    # 1) åŠ è½½é¢„å…ˆç®—å¥½çš„ image_embeddings & æ–‡æœ¬åˆ—è¡¨
    embedding_dir = os.path.abspath(
        os.path.join(os.path.dirname(__file__), "../../src/baseline/data/embedding")
    )
    image_embeddings_np, image_paths, text_items = load_embeddings(embedding_dir)
    print(f"è½½å…¥ image_embeddings: {image_embeddings_np.shape}, æ–‡æœ¬æ•°é‡: {len(text_items)}")

    # [N, D]
    image_embs = torch.from_numpy(image_embeddings_np).float().to(device)
    image_embs.requires_grad_(False)
    feat_dim = image_embs.shape[1]

    # 2) Dataset & DataLoader
    dataset = TextImageRLDataset(text_items)
    batch_size = 32
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # 3) å†»ç»“ CLIPï¼Œåªç”¨å®ƒåš feature extractor
    clip_model, _ = clip.load("ViT-B/32", device=device)
    clip_model.eval()
    for p in clip_model.parameters():
        p.requires_grad = False

    # 4) åªè®­ç»ƒä¸€ä¸ªå° adapterï¼ˆä¸ä¼šåŠ¨ CLIP åŸå§‹ç©ºé—´ï¼‰
    adapter = RankAdapter(dim=feat_dim).to(device)
    optimizer = torch.optim.Adam(adapter.parameters(), lr=LR)
    temperature = 0.1
    num_epochs = NUM_EPOCHS
    alpha = ALPHA

    identity = torch.eye(feat_dim, device=device)

    print("\n===== Start RL-inspired Rank-Aware Training (Adapter Only) =====")

    for epoch in range(num_epochs):
        adapter.train()
        epoch_losses = []
        epoch_weights = []

        pbar = tqdm(dataloader, desc=f"[Epoch {epoch+1}/{num_epochs}]")

        for indices, texts in pbar:
            indices = indices.to(device)  # [B]

            # --- 1) ç”¨ CLIP æå–æ–‡æœ¬ç‰¹å¾ï¼ˆä¸æ±‚å¯¼ï¼Œä¿è¯ç¨³å®šï¼‰ ---
            with torch.no_grad():
                tokens = clip.tokenize(list(texts)).to(device)    # [B, L]
                text_features = clip_model.encode_text(tokens)    # [B, D]
                text_features = text_features.float() 

            # --- 2) é€šè¿‡ adapter åšè½»å¾®å˜æ¢ï¼ˆåªè®­è¿™ä¸ªï¼‰ ---
            text_features = adapter(text_features)                # [B, D]

            # å½’ä¸€åŒ–ï¼Œé¿å…æ•°å€¼çˆ†ç‚¸ï¼ŒåŠ å…¥ eps é˜²æ­¢é™¤ 0
            norms = text_features.norm(dim=-1, keepdim=True) + 1e-8
            text_features = text_features / norms

            # --- 3) ä¸æ‰€æœ‰å›¾ç‰‡ç®—ç›¸ä¼¼åº¦ & log_softmax ---
            sims = text_features @ image_embs.T                  # [B, N]
            logits = sims / temperature
            log_probs = F.log_softmax(logits, dim=-1)            # [B, N]

            # --- 4) é€æ ·æœ¬è®¡ç®— rank-aware åŠ æƒ CE ---
            loss_sum = 0.0
            batch_ce = []
            batch_w = []

            B = indices.size(0)
            for i in range(B):
                true_idx = indices[i].item()          # æ­£ç¡®å›¾ç‰‡åœ¨ image_embs ä¸­çš„ç´¢å¼•

                sim_row = sims[i]                     # [N]
                logprob_row = log_probs[i]            # [N]
                log_prob_true = logprob_row[true_idx] # scalar

                # rank & inv_rank (reward=1/rank)
                rank, inv_rank = compute_rank_and_reward(sim_row, true_idx)

                # åŸºæœ¬ CEï¼š-log P(true)
                ce_i = -log_prob_true                 # scalar tensor

                # rank-aware æƒé‡ï¼šrank è¶Šå°(è¶Šå‰)ï¼Œinv_rank è¶Šå¤§ï¼Œweight è¶Šå¤§
                weight = 1.0 + alpha * inv_rank       # scalar tensor, ä»‹äº [1, 1+alpha]

                loss_sum += weight * ce_i

                batch_ce.append(ce_i.item())
                batch_w.append(weight.item())

            loss = loss_sum / B

            # L2 æ­£åˆ™ï¼šè®© adapter.weight æ¥è¿‘æ’ç­‰çŸ©é˜µ
            reg = ((adapter.linear.weight - identity) ** 2).mean()
            total_loss = loss + L2_LAMBDA * reg

            optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(adapter.parameters(), max_norm=1.0)
            optimizer.step()

            epoch_losses.append(loss.item())
            epoch_weights.extend(batch_w)

            pbar.set_postfix(
                loss=f"{np.mean(epoch_losses):.4f}",
                avg_w=f"{np.mean(epoch_weights):.3f}"
            )

        print(f">>> Epoch {epoch+1} | Avg Loss={np.mean(epoch_losses):.4f}, Avg Weight={np.mean(epoch_weights):.3f}\n")

    # 5) è®­ç»ƒç»“æŸåï¼Œç”¨ adapter é‡æ–°ç¼–ç æ‰€æœ‰æ–‡æœ¬å¹¶ä¿å­˜
    print("\n===== Encoding text features with RL-inspired Adapter =====")
    adapter.eval()
    all_embs = []

    with torch.no_grad():
        for i in range(0, len(text_items), batch_size):
            batch = text_items[i:i + batch_size]
            tokens = clip.tokenize(batch).to(device)
            text_features = clip_model.encode_text(tokens)
            text_features = text_features.float()
            text_features = adapter(text_features)
            norms = text_features.norm(dim=-1, keepdim=True) + 1e-8
            text_features = text_features / norms
            all_embs.append(text_features)

    all_text_embs = torch.cat(all_embs, dim=0).float()  # [N, D]
    save_path = os.path.join(embedding_dir, "clip_text_features_rl.npy")
    np.save(save_path, all_text_embs.cpu().numpy())
    print("ä¿å­˜ RL-inspired æ–‡æœ¬å‘é‡åˆ°:", save_path)

    # 6) ç”¨ RL-inspired æ–‡æœ¬å‘é‡åšä¸€æ¬¡ Recall@K è¯„ä¼°
    print("\n===== Evaluating Recall@K (Adapter Text vs. Original Image Embeddings) =====")
    avg_recalls = recall_at_k(all_text_embs, image_embs, k_list=(1, 5, 10))
    for k, v in avg_recalls.items():
        print(f"Recall@{k}: {v:.4f} ({v * 100:.2f}%)")

    print("\nğŸ‰ RL-inspired è®­ç»ƒå®Œæˆï¼æ¨¡å‹å’Œå‘é‡å·²ä¿å­˜ï¼Œå¯ä»¥è¿›è¡Œæ£€ç´¢å¯¹æ¯”æµ‹è¯•ã€‚")


if __name__ == "__main__":
    main()
